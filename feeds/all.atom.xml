<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Johan Carlin</title><link href="http://jooh.github.io/" rel="alternate"></link><link href="http://jooh.github.io/feeds/all.atom.xml" rel="self"></link><id>http://jooh.github.io/</id><updated>2015-12-13T21:46:36+00:00</updated><entry><title>Journal club: Estimating the dimensionality of neuronal representations during cognitive tasks</title><link href="http://jooh.github.io/journal-club-estimating-the-dimensionality-of-neuronal-representations-during-cognitive-tasks.html" rel="alternate"></link><updated>2015-12-13T21:46:36+00:00</updated><author><name>Johan Carlin</name></author><id>tag:jooh.github.io,2015-12-13:journal-club-estimating-the-dimensionality-of-neuronal-representations-during-cognitive-tasks.html</id><summary type="html">&lt;p&gt;It&amp;#8217;s a bit of a cliche that the best papers are the ones that raise more
questions than they answer (in fact, many papers seem to answer hardly
anything at all on close inspection and it doesn&amp;#8217;t mean they&amp;#8217;re great). But
I think this might be one of those papers for which the cliche holds true
in a positive sense. &lt;a href="http://dx.doi.org/10.1038/nature12160"&gt;Rigotti and
colleagues&lt;/a&gt; (2013, Nature) reported
a really intriguing re-analysis of some single-unit data from macaque &lt;span class="caps"&gt;PFC&lt;/span&gt;.
The central idea here is to attempt to estimate the dimensionality of the
neuronal representation, and to connect this to task performance. This
sounds abstract, but I think the strength of the paper lies in how the
authors frame dimensionality in terms of linear&amp;nbsp;separability.&lt;/p&gt;
&lt;p&gt;The basic idea goes like this: If we represent neuronal firing rates in some
task with a &lt;em&gt;n&lt;/em&gt; by &lt;em&gt;c&lt;/em&gt; matrix where &lt;em&gt;n&lt;/em&gt; represents cells and &lt;em&gt;c&lt;/em&gt; the unique
conditions, the most task-related dimensions that a neural representation
can encode would equal to &lt;em&gt;c&lt;/em&gt;. Ordinarily, you could take the rank of the
matrix (assuming &lt;em&gt;n=&amp;gt;c&lt;/em&gt;) to test how many dimensions are present. The rank
will be less than &lt;em&gt;c&lt;/em&gt; if some of the conditions are linear combinations of
each other. The catch is that neuroscientific data is noisy, which inflates
the dimensionality all the way up to &lt;em&gt;n&lt;/em&gt; in practically all cases. So how do
you estimate the dimensionality in the presence of&amp;nbsp;noise?&lt;/p&gt;
&lt;p&gt;Rigotti&amp;#8217;s solution is to approach the problem indirectly via linear
separability. One way to think of a representation&amp;#8217;s dimensionality is that
it&amp;#8217;s related to the number of ways in which you can bisect the space with a
discriminant. Imagine arbitrarily splitting the conditions into two classes, and
using a standard linear discriminant analysis to find a hyperplane that
separates the two classes. If the matrix is full rank, this is always
possible for all arbitrary splits of the conditions. So the number of
successful discriminants (there&amp;#8217;s &lt;em&gt;2^c&lt;/em&gt;) is related to the rank of the
matrix. This is useful because we can now deal with the noise by
cross-validating the discriminant. So the number of successful
cross-validated discriminants (and by successful, we mean accuracy over
some threshold) provides a noise-corrected measure of the dimensionality of
the underlying&amp;nbsp;representation.&lt;/p&gt;
&lt;p&gt;The most convincing evidence in the paper is in Fig 5, of which two panels
appear below. (a) shows
that the estimated dimensionality is lower for correct trials than for
error trials. By contrast, decoding of the stimulus cue is similar for
these trial types (b), which makes two points: first that it&amp;#8217;s not
that the monkey simply fell asleep on the error trials because this
stimulus distinction is present in the responses. Second, and less
intuitively, this one arguably task-relevant dimension does not distinguish
correct and error trials, while the total count over many discriminants
does, even though a good number of these splits would have very
little behavioural relevance. This is&amp;nbsp;puzzling.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 5, Rigotti et al. (2013)" src="http://jooh.github.io/images/rigotti2013.png" /&gt;&lt;/p&gt;
&lt;p&gt;A final few notes on&amp;nbsp;this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The paper has a strong spin on the topic of &amp;#8216;non-linear mixed
  selectivity&amp;#8217;, by which the authors simply mean that a neuronal code based
  on tuning to single dimensions or linear combinations thereof cannot
  support the kind of high dimensionality they observe here. Lots of
  analyses in the paper focus on removing linear selectivity and
  characterising it separately in different ways to support the case that
  non-linear tunings are essential for this. I don&amp;#8217;t think this point is as
  new or as controversial as it is presented in the&amp;nbsp;manuscript. &lt;/li&gt;
&lt;li&gt;The authors&amp;#8217; dimensionality estimation approach is neat for this
  application because it has a natural link to neuronal readout - part of
  the popularity of linear classifiers stems from the intuitive cartoon
  of the weights vector as the synaptic weights on some downstream
  representation. In this sense, a higher-dimensional representation seems
  more suited to flexible behaviour because a downstream region would be
  able to make a large number of distinctions by changing the weights. But
  there are of course many other ways to estimate the rank of noisy data
  and one wonders how this approach compares to methods used in other
  fields, where the classifier intuition is less appealing but the problem
  potentially very&amp;nbsp;similar.&lt;/li&gt;
&lt;li&gt;If &lt;span class="caps"&gt;PFC&lt;/span&gt; really furnishes such high-dimensional representations (note that
  &lt;em&gt;all&lt;/em&gt; stimulus dimensions are present in the population code according to
  Fig 5A above), why are some distinctions behaviourally more difficult
  than others? Presumably monkeys would find it much harder to learn an
  &lt;span class="caps"&gt;XOR&lt;/span&gt;-like stimulus-response mapping than a simple feature mapping, which
  doesn&amp;#8217;t seem to follow if the code were this&amp;nbsp;high-dimensional.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Rigotti, M., Barak, O., Warden, M. R., Wang, X.-J., Daw, N. D., Miller, E. K., &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Fusi, S. (2013). The importance of mixed selectivity in complex cognitive tasks. Nature, 497(7451), 585–90. &lt;a href="http://doi.org/10.1038/nature12160"&gt;http://doi.org/10.1038/nature12160&lt;/a&gt;.&lt;/p&gt;</summary><category term="pfc"></category><category term="electrophysiology"></category><category term="single-unit recording"></category><category term="low-rank matrix approximation"></category><category term="classification"></category><category term="post-publication peer review"></category></entry><entry><title>Reproducible scientific python setup with (Ana) Conda</title><link href="http://jooh.github.io/reproducible-scientific-python-setup-with-ana-conda.html" rel="alternate"></link><updated>2015-12-11T19:06:58+00:00</updated><author><name>Johan Carlin</name></author><id>tag:jooh.github.io,2015-12-11:reproducible-scientific-python-setup-with-ana-conda.html</id><summary type="html">&lt;p&gt;Getting a scientific python install up and running is still way too
complicated. In this post I describe how I use a conda to keep a
reproducible record of the packages I&amp;nbsp;use.&lt;/p&gt;
&lt;p&gt;In the past, I have usually hacked together my own developing environment
through whatever tools were most convenient (pip, github clones, built-in
packages from expansive standard python distros). This is a workable
solution if you&amp;#8217;re only doing it once, but it can be quite challenge to
achieve the exact same python environment on a new machine. This is
annoying in lots of contexts, but it&amp;#8217;s especially problematic for
scientific computing because it means others (or even your future self) may
not be able to reproduce your published&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;My solution to this issue is to use
&lt;a href="http://conda.pydata.org/docs/index.html"&gt;Conda&lt;/a&gt;, which forms an
independent part of the Anaconda python distro (I use the lighter
mini-conda). At its core, Conda is a package manager which tries to be
smart about managing your python environment. There are many competitors in
this area (the classic solution is pip combined with virtualenv, brew and
macports are other possibilities), but Conda has a few useful features that
collectively make it preferable for scientific&amp;nbsp;computing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conda packages are distributed in compiled form, which avoids all
  build-related issues (e.g., missing dependencies, broken compilers, weird
  build environments). If the package has been built properly, it is
  literally plug and play. For a taste of how smoothly this works with
  complex packages, try &lt;code&gt;conda install mayavi&lt;/code&gt; compared to &lt;code&gt;brew install
  mayavi&lt;/code&gt; and see what you&amp;nbsp;get.&lt;/li&gt;
&lt;li&gt;If a conda package is not available, it is surprisingly easy to build one
  from python libraries.  Most pip packages can be built for conda with two
  commands: &lt;code&gt;conda skeleton pypi [pip package]&lt;/code&gt; followed by &lt;code&gt;conda build
  [pip package]&lt;/code&gt;.  Adapting github repos requires a bit more manual
  editing of a &lt;span class="caps"&gt;YAML&lt;/span&gt; file but even this is simple enough (see e.g. &lt;a href="https://github.com/jooh/neuroconda/tree/master/pycortex"&gt;this
  pycortex recipe I
  wrote&lt;/a&gt;). Under
  the hood, there&amp;#8217;s quite a bit of cleverness going on with e.g. converting
  absolute paths to relative to enable this to work as smoothly as it&amp;nbsp;does.&lt;/li&gt;
&lt;li&gt;Conda includes a free package repository at
  &lt;a href="http://anaconda.org"&gt;anaconda.org&lt;/a&gt;, where users can upload packages.
  At build time, users are nudged toward setting &lt;code&gt;anaconda_upload: yes&lt;/code&gt; in
  their .condarc files, which means that any successful build is
  uploaded to your anaconda.org repo. This option appears to be popular
  because a huge number of user-built packages are available here. This is
  useful for quickly checking out more obscure packages that aren&amp;#8217;t in
  the official conda&amp;nbsp;channel.&lt;/li&gt;
&lt;li&gt;That being said, for reproducibility it is probably a better idea to
  build packages yourself and upload them to your own repository since
  other users can otherwise break your dependencies by removing or altering
  the package you&amp;#8217;re channeling. Uploading your own builds has the added
  advantage of solving your deployment issues &amp;#8212; all you have to do on a new
  machine is add your repository to the set conda will search when
  installing packages (e.g., &lt;code&gt;conda config --add channels jcarlin&lt;/code&gt;), and
  the standard conda install command will just&amp;nbsp;work.&lt;/li&gt;
&lt;li&gt;Conda&amp;#8217;s environment handling is quite good, and seems to err on the side
  of safety at the expense of disk space (ie, copy everything) compared to
  e.g. brew. I have yet to bump into any interactions between different
  environments. Generally, it&amp;#8217;s a good idea to have a different environment
  for each broad task you use python for (I use one for psychopy, one for
  neuroimaging analysis and one for web development), since packages
  sometimes require different versions of the same modules. Conda tries to
  manage such situations, but often the compromise is to downgrade core
  packages (e.g. numpy) to fairly old&amp;nbsp;versions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, my entire python environment is now made up of Conda packages,
which is neat because it means that I can reproduce my python setup
anywhere. There is a bit of overhead in going this route (especially if you
want to avoid having dependencies from other anaconda.org users), but this
should be recouped quickly down the road as the code gets deployed to
psychophysics test laptops, cloud compute, other lab&amp;nbsp;members&amp;#8230;&lt;/p&gt;</summary><category term="python"></category><category term="data science"></category><category term="conda"></category><category term="anaconda"></category><category term="scientific computing"></category><category term="virtual environment"></category></entry><entry><title>Post 1</title><link href="http://jooh.github.io/post-1.html" rel="alternate"></link><updated>2015-12-11T17:06:59+00:00</updated><author><name>Johan Carlin</name></author><id>tag:jooh.github.io,2015-12-11:post-1.html</id><summary type="html">&lt;p&gt;This is my new&amp;nbsp;website.&lt;/p&gt;</summary></entry></feed>