<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Johan Carlin</title><link href="http://www.johancarlin.com/" rel="alternate"></link><link href="http://www.johancarlin.com/feeds/all.atom.xml" rel="self"></link><id>http://www.johancarlin.com/</id><updated>2016-01-05T10:49:58+00:00</updated><entry><title>Cloud-free folder syncing with Unison</title><link href="http://www.johancarlin.com/cloud-free-folder-syncing-with-unison.html" rel="alternate"></link><updated>2016-01-05T10:49:58+00:00</updated><author><name>Johan Carlin</name></author><id>tag:www.johancarlin.com,2016-01-05:cloud-free-folder-syncing-with-unison.html</id><summary type="html">&lt;p&gt;Syncing folders between computers can be surprisingly complicated if you
work for &lt;a href="http://www.mrc.ac.uk"&gt;an academic institution that takes a dim view of Dropbox and
other cloud-based solutions&lt;/a&gt;. You are also sure to
run out of space fast if you use it to sync e.g. neuroimaging data.
&lt;a href="https://www.cis.upenn.edu/~bcpierce/unison"&gt;Unison&lt;/a&gt; is a free command-line
solution for direct computer-to-computer sync. It is basically a clever
wrapper around rsync, which takes care of all the niggly details of getting
two-way sync working properly. It also provides a nice command-line
interface for resolving sync conflicts (far better than e.g. Evernote&amp;#8217;s &lt;span class="caps"&gt;GUI&lt;/span&gt;
for the same&amp;nbsp;thing).&lt;/p&gt;
&lt;p&gt;Getting Unison up and running for your setup can be complicated. Here I
present a simple shell script that takes care of the standard case of
syncing two computers over &lt;span class="caps"&gt;SSH&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In general, my solution assumes that one computer is used to issue the
unison command (the client, which is probably your laptop or home computer)
which then connects and syncs with another computer (the remote, which is
probably a server or a desktop computer at your place of work). You could
set this up to run automatically as a cron or launchd, but I prefer to
issue the command manually at the end of the workday to make sure that I
have saved all changes&amp;nbsp;first.&lt;/p&gt;
&lt;h2&gt;Installing&amp;nbsp;unison&lt;/h2&gt;
&lt;p&gt;Unison is available through various package managers, e.g. macports and
brew. The main stumbling point here is to ensure unison is available on the
remote (the computer you are syncing to). On my setup (brew with &lt;span class="caps"&gt;OS&lt;/span&gt; X
Yosemite), unison returned &lt;code&gt;bash: unison:
command not found&lt;/code&gt;, even though unison was clearly availabe when connecting
to the remote over &lt;span class="caps"&gt;SSH&lt;/span&gt;. By contrast, &lt;code&gt;ssh [client] unison&lt;/code&gt; returned an
error, thus revealing that unison was only getting
added to the path in interactive shells (&lt;a href="http://unix.stackexchange.com/questions/79723/why-do-ssh-host-echo-path-and-printing-the-path-after-sshing-into-the-machi"&gt;which are not the same as login
shells used by scripts&lt;/a&gt;). In the end I manually added
/usr/local/bin to my path in .bashrc to get around this issue
(.bash_profile wouldn&amp;#8217;t work since this isn&amp;#8217;t sourced by non-interactive&amp;nbsp;shells).&lt;/p&gt;
&lt;h2&gt;Configuring&amp;nbsp;unison&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://gist.github.com/jooh/bbdf2b311017b3453830"&gt;unisync&lt;/a&gt; script
below provides the basic sync functionality for syncing between two folders
named unibox in the user&amp;#8217;s home directory. The first input argument
specifies the hostname, while the second provides the username for the
remote. By default, the username for client and remote as assumed to be the&amp;nbsp;same.&lt;/p&gt;
&lt;div class="gist"&gt;
    &lt;script src='https://gist.github.com/bbdf2b311017b3453830.js'&gt;&lt;/script&gt;
    &lt;noscript&gt;
        &lt;pre&gt;&lt;code&gt;#!/bin/tcsh
#
#       by: Johan Carlin
#     date: 8/9/2014
#  purpose: sync folders between computers. Assumes that both are available
#   on the local network (ie you are using VPN if the host is behind a
#   firewall)
#    usage: unisync [ucl] [j.carlin]

# get optional input
if ($#argv &gt;= 1) then
  set hostname="$1"
else
  set hostname=ucl
endif

if ($#argv == 2) then
  set touser="$2"
else
  set touser=$USER
endif

set fromdir=/Users/$USER/unibox
set todir=/Users/$touser/unibox

# auto should mean that we get batch mode if there are no conflicts, and
# interactive mode otherwise
unison $fromdir "ssh://"$hostname"/"$todir -auto
&lt;/code&gt;&lt;/pre&gt;
    &lt;/noscript&gt;
&lt;/div&gt;
&lt;p&gt;So for instance, my standard usage is &lt;code&gt;unisync ucl j.carlin&lt;/code&gt;, where ucl is
an alias or shortcut for my full &lt;span class="caps"&gt;SSH&lt;/span&gt; connection stored in ssh/.config, and
j.carlin is my username which is necessary here because it is different
from my username on the&amp;nbsp;client.&lt;/p&gt;</summary><category term="shell"></category><category term="scientific computing"></category></entry><entry><title>Journal club: Estimating the dimensionality of neuronal representations during cognitive tasks</title><link href="http://www.johancarlin.com/journal-club-estimating-the-dimensionality-of-neuronal-representations-during-cognitive-tasks.html" rel="alternate"></link><updated>2016-01-05T10:04:41+00:00</updated><author><name>Johan Carlin</name></author><id>tag:www.johancarlin.com,2016-01-05:journal-club-estimating-the-dimensionality-of-neuronal-representations-during-cognitive-tasks.html</id><summary type="html">&lt;p&gt;It&amp;#8217;s a bit of a cliche that the best papers are the ones that raise more
questions than they answer (in fact, many papers seem to answer hardly
anything at all on close inspection and it doesn&amp;#8217;t mean they&amp;#8217;re great). But
I think this might be one of those papers for which the cliche holds true
in a positive sense. &lt;a href="http://dx.doi.org/10.1038/nature12160"&gt;Rigotti and
colleagues&lt;/a&gt; (2013, Nature) reported
a really intriguing re-analysis of some single-unit data from macaque &lt;span class="caps"&gt;PFC&lt;/span&gt;.
The central idea here is to attempt to estimate the dimensionality of the
neuronal representation, and to connect this to task performance. This
sounds abstract, but I think the strength of the paper lies in how the
authors frame dimensionality in terms of linear&amp;nbsp;separability.&lt;/p&gt;
&lt;p&gt;The basic idea goes like this: If we represent neuronal firing rates in some
task with a &lt;em&gt;n&lt;/em&gt; by &lt;em&gt;c&lt;/em&gt; matrix where &lt;em&gt;n&lt;/em&gt; represents cells and &lt;em&gt;c&lt;/em&gt; the unique
conditions, the most task-related dimensions that a neural representation
can encode would equal to &lt;em&gt;c&lt;/em&gt;. Ordinarily, you could take the rank of the
matrix (assuming &lt;em&gt;n=&amp;gt;c&lt;/em&gt;) to test how many dimensions are present. The rank
will be less than &lt;em&gt;c&lt;/em&gt; if some of the conditions are linear combinations of
each other. The catch is that neuroscientific data is noisy, which inflates
the dimensionality all the way up to &lt;em&gt;n&lt;/em&gt; in practically all cases. So how do
you estimate the dimensionality in the presence of&amp;nbsp;noise?&lt;/p&gt;
&lt;p&gt;Rigotti&amp;#8217;s solution is to approach the problem indirectly via linear
separability. One way to think of a representation&amp;#8217;s dimensionality is that
it&amp;#8217;s related to the number of ways in which you can bisect the space with a
discriminant. Imagine arbitrarily splitting the conditions into two classes, and
using a standard linear discriminant analysis to find a hyperplane that
separates the two classes. If the matrix is full rank, this is always
possible for all arbitrary splits of the conditions. So the number of
successful discriminants (there&amp;#8217;s &lt;em&gt;2^c&lt;/em&gt;) is related to the rank of the
matrix. This is useful because we can now deal with the noise by
cross-validating the discriminant. So the number of successful
cross-validated discriminants (and by successful, we mean accuracy over
some threshold) provides a noise-corrected measure of the dimensionality of
the underlying&amp;nbsp;representation.&lt;/p&gt;
&lt;p&gt;The most convincing evidence in the paper is in Fig 5, of which two panels
appear below. (a) shows
that the estimated dimensionality is lower for correct trials than for
error trials. By contrast, decoding of the stimulus cue is similar for
these trial types (b), which makes two points: first that it&amp;#8217;s not
that the monkey simply fell asleep on the error trials because this
stimulus distinction is present in the responses. Second, and less
intuitively, this one arguably task-relevant dimension does not distinguish
correct and error trials, while the total count over many discriminants
does, even though a good number of these splits would have very
little behavioural relevance. This is&amp;nbsp;puzzling.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 5, Rigotti et al. (2013)" src="http://www.johancarlin.com/images/rigotti2013.png" /&gt;&lt;/p&gt;
&lt;p&gt;A final few notes on&amp;nbsp;this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The paper has a strong spin on the topic of &amp;#8216;non-linear mixed
  selectivity&amp;#8217;, by which the authors simply mean that a neuronal code based
  on tuning to single dimensions or linear combinations thereof cannot
  support the kind of high dimensionality they observe here. Lots of
  analyses in the paper focus on removing linear selectivity and
  characterising it separately in different ways to support the case that
  non-linear tunings are essential for this. I don&amp;#8217;t think this point is as
  new or as controversial as it is presented in the&amp;nbsp;manuscript. &lt;/li&gt;
&lt;li&gt;The authors&amp;#8217; dimensionality estimation approach is neat for this
  application because it has a natural link to neuronal readout - part of
  the popularity of linear classifiers stems from the intuitive cartoon
  of the weights vector as the synaptic weights on some downstream
  representation. In this sense, a higher-dimensional representation seems
  more suited to flexible behaviour because a downstream region would be
  able to make a large number of distinctions by changing the weights. But
  there are of course many other ways to estimate the rank of noisy data
  and one wonders how this approach compares to methods used in other
  fields, where the classifier intuition is less appealing but the problem
  potentially very&amp;nbsp;similar.&lt;/li&gt;
&lt;li&gt;If &lt;span class="caps"&gt;PFC&lt;/span&gt; really furnishes such high-dimensional representations (note that
  &lt;em&gt;all&lt;/em&gt; stimulus dimensions are present in the population code according to
  Fig 5A above), why are some distinctions behaviourally more difficult
  than others? Presumably monkeys would find it much harder to learn an
  &lt;span class="caps"&gt;XOR&lt;/span&gt;-like stimulus-response mapping than a simple feature mapping, which
  doesn&amp;#8217;t seem to follow if the code were this&amp;nbsp;high-dimensional.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;p&gt;Rigotti, M., Barak, O., Warden, M. R., Wang, X.-J., Daw, N. D., Miller, E. K., &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Fusi, S. (2013). The importance of mixed selectivity in complex cognitive tasks. Nature, 497(7451), 585–90. &lt;a href="http://doi.org/10.1038/nature12160"&gt;http://doi.org/10.1038/nature12160&lt;/a&gt;.&lt;/p&gt;</summary><category term="pfc"></category><category term="electrophysiology"></category><category term="single-unit recording"></category><category term="low-rank matrix approximation"></category><category term="classification"></category><category term="post-publication peer review"></category></entry><entry><title>Reproducible scientific python setup with (Ana) Conda</title><link href="http://www.johancarlin.com/reproducible-scientific-python-setup-with-ana-conda.html" rel="alternate"></link><updated>2015-12-11T19:06:58+00:00</updated><author><name>Johan Carlin</name></author><id>tag:www.johancarlin.com,2015-12-11:reproducible-scientific-python-setup-with-ana-conda.html</id><summary type="html">&lt;p&gt;Getting a scientific python install up and running is still way too
complicated. In this post I describe how I use a conda to keep a
reproducible record of the packages I&amp;nbsp;use.&lt;/p&gt;
&lt;p&gt;In the past, I have usually hacked together my own developing environment
through whatever tools were most convenient (pip, github clones, built-in
packages from expansive standard python distros). This is a workable
solution if you&amp;#8217;re only doing it once, but it can be quite challenge to
achieve the exact same python environment on a new machine. This is
annoying in lots of contexts, but it&amp;#8217;s especially problematic for
scientific computing because it means others (or even your future self) may
not be able to reproduce your published&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;My solution to this issue is to use
&lt;a href="http://conda.pydata.org/docs/index.html"&gt;Conda&lt;/a&gt;, which forms an
independent part of the Anaconda python distro (I use the lighter
mini-conda). At its core, Conda is a package manager which tries to be
smart about managing your python environment. There are many competitors in
this area (the classic solution is pip combined with virtualenv, brew and
macports are other possibilities), but Conda has a few useful features that
collectively make it preferable for scientific&amp;nbsp;computing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conda packages are distributed in compiled form, which avoids all
  build-related issues (e.g., missing dependencies, broken compilers, weird
  build environments). If the package has been built properly, it is
  literally plug and play. For a taste of how smoothly this works with
  complex packages, try &lt;code&gt;conda install mayavi&lt;/code&gt; compared to &lt;code&gt;brew install
  mayavi&lt;/code&gt; and see what you&amp;nbsp;get.&lt;/li&gt;
&lt;li&gt;If a conda package is not available, it is surprisingly easy to build one
  from python libraries.  Most pip packages can be built for conda with two
  commands: &lt;code&gt;conda skeleton pypi [pip package]&lt;/code&gt; followed by &lt;code&gt;conda build
  [pip package]&lt;/code&gt;.  Adapting github repos requires a bit more manual
  editing of a &lt;span class="caps"&gt;YAML&lt;/span&gt; file but even this is simple enough (see e.g. &lt;a href="https://github.com/jooh/neuroconda/tree/master/pycortex"&gt;this
  pycortex recipe I
  wrote&lt;/a&gt;). Under
  the hood, there&amp;#8217;s quite a bit of cleverness going on with e.g. converting
  absolute paths to relative to enable this to work as smoothly as it&amp;nbsp;does.&lt;/li&gt;
&lt;li&gt;Conda includes a free package repository at
  &lt;a href="http://anaconda.org"&gt;anaconda.org&lt;/a&gt;, where users can upload packages.
  At build time, users are nudged toward setting &lt;code&gt;anaconda_upload: yes&lt;/code&gt; in
  their .condarc files, which means that any successful build is
  uploaded to your anaconda.org repo. This option appears to be popular
  because a huge number of user-built packages are available here. This is
  useful for quickly checking out more obscure packages that aren&amp;#8217;t in
  the official conda&amp;nbsp;channel.&lt;/li&gt;
&lt;li&gt;That being said, for reproducibility it is probably a better idea to
  build packages yourself and upload them to your own repository since
  other users can otherwise break your dependencies by removing or altering
  the package you&amp;#8217;re channeling. Uploading your own builds has the added
  advantage of solving your deployment issues &amp;#8212; all you have to do on a new
  machine is add your repository to the set conda will search when
  installing packages (e.g., &lt;code&gt;conda config --add channels jcarlin&lt;/code&gt;), and
  the standard conda install command will just&amp;nbsp;work.&lt;/li&gt;
&lt;li&gt;Conda&amp;#8217;s environment handling is quite good, and seems to err on the side
  of safety at the expense of disk space (ie, copy everything) compared to
  e.g. brew. I have yet to bump into any interactions between different
  environments. Generally, it&amp;#8217;s a good idea to have a different environment
  for each broad task you use python for (I use one for psychopy, one for
  neuroimaging analysis and one for web development), since packages
  sometimes require different versions of the same modules. Conda tries to
  manage such situations, but often the compromise is to downgrade core
  packages (e.g. numpy) to fairly old&amp;nbsp;versions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, my entire python environment is now made up of Conda packages,
which is neat because it means that I can reproduce my python setup
anywhere. There is a bit of overhead in going this route (especially if you
want to avoid having dependencies from other anaconda.org users), but this
should be recouped quickly down the road as the code gets deployed to
psychophysics test laptops, cloud compute, other lab&amp;nbsp;members&amp;#8230;&lt;/p&gt;</summary><category term="python"></category><category term="data science"></category><category term="conda"></category><category term="anaconda"></category><category term="scientific computing"></category><category term="virtual environment"></category></entry><entry><title>Post 1</title><link href="http://www.johancarlin.com/post-1.html" rel="alternate"></link><updated>2015-12-11T17:06:59+00:00</updated><author><name>Johan Carlin</name></author><id>tag:www.johancarlin.com,2015-12-11:post-1.html</id><summary type="html">&lt;p&gt;This is my new&amp;nbsp;website.&lt;/p&gt;</summary></entry></feed>